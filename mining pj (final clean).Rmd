---
title: "Data Cleaning"
output: html_notebook
---


Load library
```{r}
rm(list=ls(all=TRUE))
library(pacman)
p_load(readxl, e1071 ,dplyr,stringr, tidyverse, psych, caret, MLmetrics, skimr,
       car, BiocManager, caTools, fmsb, pROC, randomForest,
       caret, fpp2, tidyverse, zoo, rpart, rpart.plot, neuralnet, pROC, nnet,
       lubridate, qdapTools, qdapDictionaries, randomForest, ranger, doParallel)
```


Load data
```{r}
dat <- read.csv('dataset.csv')
head(dat)
```


Check missing data
```{r}
sum(is.na(dat))
```


Remove duplicates
```{r}
dat <- dat[!duplicated(dat$password), ]
nrow(dat)
```


Check data types
```{r}
lapply(dat, class)
```


Fix the misspelling of "weak" in our dataset
```{r}
dat[dat$class_strength == "Week", "class_strength"] <- "Weak"
dat[dat$class_strength == "Very week", "class_strength"] <- "Very weak"
unique(dat$class_strength)
```


Check strength distribution
```{r}
hist(dat$strength)
```


Add new variables
Creating new variables is an important step in our analysis
We create several new variables to support our models, including:
- numNumber: count of numeric characters in each password string
- numAlphabet: count of alphabet characters in each password string
- numSpecial: count of special characters in each password string
- numSpecial: count of capital characters in each password string
- numUnique:count of unique character types in each password string
```{r}
dat$numNumber <- str_count(dat$password, "[0-9]")
dat$numAlphabet <- str_count(str_to_lower(dat$password), "[a-z]")
dat$numSpecial <- str_count(dat$password, "[^A-Za-z0-9]")
dat$numCapital <- str_count(dat$password, "[A-Z]")
dat$numUnique <- sapply(strsplit(dat$password, ""), function(x) length(unique(x)))
head(dat)
```


Create isDict variable: whether the password string is a word in dictionary
```{r}
dat$isDict <- ifelse(str_to_lower(dat$password) %in% GradyAugmented, 1, 0)
head(dat[dat$isDict == 1,])
```


Create new variable isCommon, which is the number of times the password string appears in rockyou and xatonet.
These are 2 external datasets we downloaded to support our main dataset. Both include the most common passwords in the respective network.
First, we load the datasets in R.
```{r}
common_pass <- readLines("xato-net-10-million-passwords.txt") %>%
  as.list() %>%
  set_names(., .)
rockyou <- readLines("rockyou.txt") %>%
  as.list() %>%
  set_names(., .)
```


Next, count the appearance in the external datasets.
```{r}
dat$isCommon <- as.factor(ifelse(dat$password %in% common_pass, 1, 0) + 
                          ifelse(dat$password %in% rockyou, 1, 0))
dat %>%
  group_by(isCommon) %>%
  summarise(n())
```


Split data into training and validation.
We use training_set variable to denote training set (training_set = 1) and validation set (training_set = 0).
This can ensure we have the same train/validation split even if we split the dataset on different operating systems or computers.
```{r}
set.seed(7)
train.index <- sample(row.names(dat), 0.7*dim(dat)[1]) 
valid.index <- setdiff(row.names(dat), train.index) 
dat[train.index, "train_set"] <- 1
dat[valid.index, "train_set"] <- 0

dat %>%
  group_by(train_set) %>%
  summarise(n())
```


Export new csv
```{r}
write.csv(dat, "clean_password_1129.csv")
```