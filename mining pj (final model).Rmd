---
title: "Predictive Models"
output: html_notebook
---


This project is multiclass classification.
First, we choose this approach to improve our interpretability. From the user's perspective, they only want to know if their password is strong/weak, and unlikely to want to know strength score.
Besides, it's also about challenge and experience. It's a pretty interesting and rare experience to work with multiclass classification, which is much rarer than 2-class classification. We can a lot about adapting our codes to multiclass problems, especially for multiclass logistic regression.


Load library
```{r}
rm(list=ls(all=TRUE))
library(pacman)
p_load(readxl, e1071 ,dplyr,stringr, tidyverse, psych, caret, MLmetrics, skimr,
       car, BiocManager, caTools, fmsb, pROC, randomForest,
       caret, fpp2, tidyverse, zoo, rpart, rpart.plot, neuralnet, pROC, nnet,
       lubridate, qdapTools, qdapDictionaries, randomForest, ranger, doParallel)
```


Load clean dataset:
```{r}
dat_raw <- read.csv('clean_password_1129.csv')
dat_raw$class_strength <- as.factor(make.names(dat_raw$class_strength))
dat_raw$crack_time <- as.factor(gsub("[^A-Za-z]", "", dat_raw$crack_time))

dat_reg <- dat_raw %>% select(-X, -password, -class_strength)
dat <- dat_raw %>% select(-X, -password, -strength)
head(dat$class_strength)
```


Check data distribution:
```{r}
dat %>%
  group_by(class_strength) %>%
  summarise(count = n())
```


Split the dataset into training set and validation set, based on training_set variable
```{r}
train <- dat[dat$train_set == 1,] %>% select(-train_set)
train_reg <- dat_reg[dat_reg$train_set == 1,] %>% select(-train_set)
val <- dat[dat$train_set == 0,] %>% select(-train_set)
val_reg <- dat_reg[dat_reg$train_set == 0,] %>% select(-train_set)

norm.values <- preProcess(train, method= c("scale", "center"))
train_norm <- predict(norm.values, train)
val_norm <- predict(norm.values, val)

cat(nrow(train), " ", nrow(val))
```


Check training set
```{r}
sum(is.na(train))
levels(train$class_strength)
train %>%
  group_by(class_strength) %>%
  summarise(count = n())
```


Check test set
```{r}
val %>%
  group_by(class_strength) %>%
  summarise(count = n())
```


Running random forest model
- mtry: the number of variables each tree can look at. 
- nodesize: minimum terminal node size
We choose one that maximizes predictive performance on validation set.
```{r}
set.seed(7)
rf <- randomForest(class_strength ~ ., data = train, 
                   mtry = 3, nodesize = 1,
                   importance = TRUE)
summary(rf)
```


Variable importance ranking from random forest
```{r}
rfVarImp <- varImp(rf)
rfVarImp
```


Prediction and confusion matrix from random forest
```{r}
rf_pred <- predict(rf, val)
res <- data.frame(rf_pred, val$class_strength)
confusionMatrix(rf_pred, val$class_strength)
```


Sanity check result of random forest
```{r}
res %>%
  filter(val.class_strength == "Strong") %>%
  head(5)
```
```{r}
res %>%
  filter(val.class_strength != rf_pred) %>%
  head(20)
```


Training decision tree
```{r}
tr <- rpart(class_strength ~., data=train)
prp(tr)
```


Pruned tree: We attempt to prune the tree, but our unpruned tree already achieves the best CV performance.
```{r}
tr2 <- prune(tr, cp = tr$cptable[which.min(tr$cptable[,'xerror']), 'CP'])
prp(tr2)
```


Prediction and confusion matrix for decision tree
```{r}
tr_prob <- predict(tr,val)
tr_pred <- as.factor(colnames(tr_prob)[apply(tr_prob,1,which.max)])
confusionMatrix(tr_pred, as.factor(val$class_strength))
```


Setup random forest regression cross-validation hyperparameter tuning:
We attempt to use cross-validation to optimize our model. Because models such as random forest or XGBoost depends a lot on the choice of hyperparameter, using cross-validation to improve this process can really help.
We decide to use CV hyperparameter tuning on random forest regression (as opposed to random forest classification), because it's more accurate to tune hyperparameters on RMSE metric, given that our dataset is multi-class and cant use sensitivity metric well.
```{r}
grid <- expand.grid(mtry = seq(13, 18, by=1), 
                    splitrule = c("variance", "extratrees"), 
                    min.node.size = c(1,2))

set.seed(0)
rfrSeeds_df <- c()
for (row in seq(1, 3)) {
  rfrSeeds <- vector(mode = "list", length = 5) # length = (total number of folds) + 1
  for(i in 1:length(rfrSeeds)) rfrSeeds[[i]]<- sample.int(n=100, 24) 
  # 24 is the total number of tuning grids: 6 x 2 x 2
  rfrSeeds[[length(rfrSeeds)]] <- 0 
  rfrSeeds_df <- rbind(rfrSeeds_df, rfrSeeds)
}
remove(i, row)
```


Tune random forest regression. We tune 4 important hyperparameters:
- mtry: the number of variables each tree can look at
- split rule: how data is split in each node
- min node size
- depth: max tree depth
Because depth isn't included in caret package for hyperparameter tuning, we also have a manual loop to tune this hyperparameter.
We use 4 folds for our cross-validation.
```{r}
rfr_res <- data.frame(matrix(ncol = 5, nrow = 0))
min_rmse <- Inf; row <- 1
colnames(rfr_res) <- c('depth', 'mtry', 'splitrule', 'min.node.size', 'rmse')

# Parallel processing
cl <- makeCluster(detectCores(), type='PSOCK')
registerDoParallel(cl)

for (depth in seq(30, 30, 5)) {
  control <- trainControl(method="repeatedcv", number = 4, repeats = 1, seeds = rfrSeeds_df[row+2,])
  rfrFit_temp <- caret::train(strength ~ .,
                 data=train_reg, method="ranger", trControl=control,
                 tuneGrid = grid, max.depth = depth, num.trees = 30, importance = "permutation",
                 preProcess = c("center", "scale"), metric = "RMSE")

  rfr_res[nrow(rfr_res) + 1,] <- c(depth, rfrFit_temp$finalModel$tuneValue$mtry,
                                 as.character(rfrFit_temp$finalModel$tuneValue$splitrule),
                                 rfrFit_temp$finalModel$tuneValue$min.node.size,
                                 rfrFit_temp$results[[which.min(rfrFit_temp$results[, "RMSE"]),'RMSE']])

  if (rfr_res[nrow(rfr_res),]$rmse < min_rmse) {
    min_rmse <- rfr_res[nrow(rfr_res),]$rmse
    rfrFit <- rfrFit_temp
  }
  row <- row + 1
}

# Turn off parallel processing
stopCluster(cl)
remove(depth, rfrFit_temp, row)

# More compact result presentation: getTrainPerf(rfFit)
best_rfr_res <- rfr_res[which.min(rfr_res$rmse), ]
print(best_rfr_res)
remove(rfrFit)
```


Training random forest regression on the full training data (without cross-validation)
```{r}
grid <- expand.grid(mtry = 18, 
                    splitrule = "variance", 
                    min.node.size = 1)
control <- trainControl(method="none")

rfr_fin <- caret::train(strength ~ ., data=train_reg, method="ranger", 
                 trControl=control, importance = "permutation",
                 tuneGrid = grid, max.depth = 30, num.trees = 30, 
                 preProcess = c("center", "scale"), metric = "RMSE")

summary(rfr_fin)
```

Get predictions from random forest regression
```{r}
rfr_prob <- predict(rfr_fin, val)
```


Confusion matrix from random forest regression
```{r}
val_classstrength <- as.factor(val$class_strength)
rfr_pred <- as.factor(case_when(
  rfr_prob >= 0.8 ~ "Very.strong",
  rfr_prob >= 0.6 ~ "Strong",
  rfr_prob >= 0.4 ~ "Average",
  rfr_prob >= 0.2 ~ "Weak",
  TRUE ~ "Very.weak"
))
rfr_res_df <- data.frame(rfr_pred, val_classstrength)
confusionMatrix(rfr_pred, val_classstrength)
```


Sanity check predictions of random forest regression
```{r}
rfr_res_df %>%
  filter(val_classstrength == "Strong") %>%
  head(5)
```
```{r}
rfr_res_df %>%
  filter(val_classstrength != rfr_pred) %>%
  head(20)
```


Variable importance from random forest regression
```{r}
rfr_varimp <- varImp(rfr_fin)
rfr_varimp
```


Training our logistic regression
```{r}
set.seed(7)
lg <- multinom(class_strength ~ ., data = train_norm)
summary(lg)
```


Extract p-value of logistic regression
```{r}
pValue_extract <- function(x){
  z <- summary(x)$coefficients/summary(x)$standard.errors
  p <- (1 - pnorm(abs(z), 0, 1)) * 2
  p
}
pValue_extract(lg)
```


Make predictions from logistic regression
```{r}
lg_pred <- as.factor(predict(lg, newdata = val_norm, "class"))
```


Confusion matrix from logistic regression
```{r}
val_classstrength <- as.factor(val_classstrength)
confusionMatrix(lg_pred, val_classstrength)
```


Instead of using an external package, we also attempt to train multiclass logistic model from the package taught in class, which neutralnet package. 
But first, we need to convert factor variables to suitable format for neural net.
The function class.ind() is provided by neuralnet package itself
```{r}
train_m <- cbind(train_norm[, -c(2,5)], 
                 class.ind(as.factor(train_norm$class_strength)),
                 class.ind(as.factor(train_norm$crack_time)))
val_m <- cbind(val_norm[, -c(2,5)],
               class.ind(as.factor(val_norm$class_strength)),
               class.ind(as.factor(val_norm$crack_time)))

colnames(train_m)
```


Training the multiclass logistic regression.
For an equivalent logistic regression, we need to set:
- hidden = 0
- error function is cross-entropy loss (ce)
- activation function is logistic
- linear output = FALSE
Besides, we need to choose our threshold:
- Low threshold: more accurate model, but resource intensive
- High threshold: less accurate model, but faster runtime
- lifesign = full: keep track of the change in our error, to choose the best threshold for our resource
We choose the threshold our computer can afford.
Although we might not get as good model as the package, it's an insightful practice.
```{r}
lg2 <- neuralnet(Very.strong + Strong + Average + Weak + Very.weak
                   ~ length + entropy + crack_time_sec + numNumber +
                   numAlphabet + numSpecial + numCapital + numUnique +
                   isDict + centuries + days + Eternity + hours +
                   instant + minutes + months + seconds + years,
                 data = train_m, linear.output	= F, 
                 lifesign = "full", threshold = 0.3, 
                 hidden = 0, err.fct = 'ce', act.fct = 'logistic')
plot(lg2)
```


Confusion matrix from self-built logistic regression
```{r}
lg_prob2 <- predict(lg2,val_m)
colnames(lg_prob2) <- c("Very.strong", "Strong", "Average", "Weak", "Very.weak")
lg_pred2 <- as.factor(colnames(lg_prob2)[apply(lg_prob2,1,which.max)])
val_classstrength <- as.factor(val_classstrength)
confusionMatrix(lg_pred2, val_classstrength)
```


Train KNN model, and get confusion matrix
```{r}
kn_pred <- class::knn(train=train_norm[,-c(2,5)],
                      test=val_norm[,-c(2,5)],
                      cl=train_norm[,2], k=10, prob=TRUE)
confusionMatrix(kn_pred, val_classstrength)

#run k-means with known of 5. 
km_result<-stats::kmeans(passwd_train_df[,-c(1,4, 14)], centers = 5)
passwd_train_df$cluster <- km_result$cluster

#return the references of the data points for the different clusters and analyse them.
which(passwd_train_df$cluster==1)
which(passwd_train_df$cluster==2)
which(passwd_train_df$cluster==3)
head(which(passwd_train_df$cluster==5))
which(passwd_train_df$cluster==4)
```


Ensemble model
We also want to try this method taught in class and tutorial exercise.
We combine the results of all models, and take their majority vote.
```{r}
all_preds <- data.frame(cbind(
  lg = lg_pred,
  rf = rf_pred,
  rfr = rfr_pred,
  tr = tr_pred,
  knn = kn_pred
))
head(all_preds)
```


Count the votes of each class:
```{r}
all_preds$Average <- rowSums(all_preds[,1:5] == 1)
all_preds$Strong <- rowSums(all_preds[,1:5] == 2)
all_preds$Very.strong <- rowSums(all_preds[,1:5] == 3)
all_preds$Very.weak <- rowSums(all_preds[,1:5] == 4)
all_preds$Weak <- rowSums(all_preds[,1:5] == 5)
head(all_preds)
```


Get the majority-voted label for each password:
```{r}
all_preds$majority <- as.factor(colnames(all_preds[,6:10])
                                [apply(all_preds[,6:10],1,which.max)])
head(all_preds)
```


Sanity check majority vote
```{r}
unique(all_preds$majority)
```


Confusion matrix from ensemble method
```{r}
confusionMatrix(all_preds$majority, val_classstrength)
```

